---
title: "merged_wrokflow"
author: "Brendan F. Miller"
date: "2/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}

k_ <- seq(from = 10, to = 75, by = 5)
k_

```


get datasets first, then come up with a common gene set for all corpus' before running models

# ==========================
# MOB

# mOB merignue data

```{r}

data(mOB)
cd <- mOB$counts

```

```{r}

mob <- preprocess(t(cd),
                  alignFile = NA, extractPos = FALSE,
                  nTopGenes = 5,
                  genes.to.remove = c("mt-"),
                  perc.spots = .95,
                  min.reads = 100,
                  min.lib.size = 100,
                  od.genes.alpha = 0.10)

mob$pos <- mOB$pos[rownames(mob$corpus),]

```

approx 5 minutes for 20-75 using corpus of 196 OD genes

```{r}

start_time <- Sys.time()

mob_LDAs <- fitLDA(mob$slm, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

could test out different corpus's and how they affect K and perplexity...

```{r}

mobLDA_alphas <- sapply(mob_LDAs$models, slot, "alpha")
dat <- data.frame(k = as.character(k_),
                  alpha = mobLDA_alphas,
                  perplexities = mob_LDAs$perplexities,
                  pos = seq(length(k_)))

ggplot() +
  geom_point(data = dat, aes(x = pos, y = perplexities)) +
  scale_x_continuous(breaks = dat$pos, labels = dat$k) +
  xlab("K") + ylab("perplexity") +
  theme_classic()

ggplot() +
  geom_point(data = dat, aes(x = pos, y = alpha)) +
  scale_x_continuous(breaks = dat$pos, labels = dat$k) +
  xlab("K") + ylab("alpha") +
  theme_classic()

```

```{r}

mob_opt <- buildLDAobject(LDAmodel = optimalModel(mob_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

```

```{r, fig.height=6, fig.width=8}

vizAllTopics(theta = mob_opt$thetaCombn,
             pos = mob$pos,
             topicOrder = seq_len(length(colnames(mob_opt$thetaCombn))),
             cluster_cols = levels(mob_opt$clustCols),
             groups = NA,
             group_cols = NA, 
             r = 0.4,
             lwd = 0.01,
             plotTitle = "mob_opt combined")

vizTopicClusters(theta = mob_opt$thetaCombn,
                pos = mob$pos,
                clusters = mob_opt$clustCols,
                sharedCol = TRUE,
                r = 0.4,
                lwd = 0.01)

vizTopicClusters(theta = mob_opt$theta,
                pos = mob$pos,
                clusters = mob_opt$cols,
                sharedCol = FALSE,
                r = 0.4,
                lwd = 0.01)

```

# Rep1 Stahl data

```{r}

path <- "/Users/brendan/Desktop/PostDoc/data/ImputeTranscriptomeFromHE/2016_Stahl/SpotGeneCountMatrices/Rep1_MOB_count_matrix-1.tsv"

mob_rep1 <- preprocess(path,
                  alignFile = NA, extractPos = TRUE,
                  nTopGenes = 5,
                  genes.to.remove = c("mt-"),
                  perc.spots = .95,
                  min.reads = 100,
                  min.lib.size = 100,
                  od.genes.alpha = 0.10)

```

```{r}

start_time <- Sys.time()

mob_rep1_LDAs <- fitLDA(mob_rep1$slm, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

```{r}

mob_rep1_opt <- buildLDAobject(LDAmodel = optimalModel(mob_rep1_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

```

# Rep2 Stahl data

```{r}

path <- "/Users/brendan/Desktop/PostDoc/data/ImputeTranscriptomeFromHE/2016_Stahl/SpotGeneCountMatrices/Rep2_MOB_count_matrix-1.tsv"

mob_rep2 <- preprocess(path,
                  alignFile = NA, extractPos = TRUE,
                  nTopGenes = 5,
                  genes.to.remove = c("mt-"),
                  perc.spots = .95,
                  min.reads = 100,
                  min.lib.size = 100,
                  od.genes.alpha = 0.10)

```

```{r}

start_time <- Sys.time()

mob_rep2_LDAs <- fitLDA(mob_rep2$slm, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

```{r}

mob_rep2_opt <- buildLDAobject(LDAmodel = optimalModel(mob_rep2_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

```

# Rep3 Stahl data

```{r}

path <- "/Users/brendan/Desktop/PostDoc/data/ImputeTranscriptomeFromHE/2016_Stahl/SpotGeneCountMatrices/Rep3_MOB_count_matrix-1.tsv"

mob_rep3 <- preprocess(path,
                  alignFile = NA, extractPos = TRUE,
                  nTopGenes = 5,
                  genes.to.remove = c("mt-"),
                  perc.spots = .95,
                  min.reads = 100,
                  min.lib.size = 100,
                  od.genes.alpha = 0.10)

```

```{r}

start_time <- Sys.time()

mob_rep3_LDAs <- fitLDA(mob_rep3$slm, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

```{r}

mob_rep3_opt <- buildLDAobject(LDAmodel = optimalModel(mob_rep3_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

```


# Standardize corpus for datasets

Common set of genes to use for all curpuses with the MOB. Make it easier to assess reliability of models and produced topics for a given tissue across different samples.

Should still be careful about the time it takes based on the copus size...



# ==========================
# PDAC

# PDAC A ST1

```{r}

path <- "/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/moncada_pdac_GSE111672/GSE111672_RAW/GSM3036911_PDAC-A-ST1-filtered.txt"

# reformat the data before preprocessing
# need a matrix in which:
pdac_a1 <- read.table(path, header = TRUE, sep = "\t")
pdac_a1

```

input for `preprocess` should be spot (row) x gene (columns) mtx with raw gene counts

```{r}

spotids <- unlist(lapply(colnames(pdac_a1)[2:ncol(pdac_a1)], function(i) {
  ix <- strsplit(i, "X")[[1]][2]
  ix
}))

t <- t(pdac_a1)
colnames(t) <- pdac_a1$Genes
t <- t[2:nrow(t),]
pdac_a1 <- apply(t, 2,FUN = as.numeric)
rownames(pdac_a1) <- spotids
pdac_a1[1:10,1:10]

```

```{r}

pdac_a1 <- preprocess(pdac_a1,
                  alignFile = NA, extractPos = TRUE,
                  nTopGenes = 5,
                  genes.to.remove = c("^MT", "^RPL", "^MRPL"), # note, MTOR is removed...maybe other non MT genes...?
                  perc.spots = .95,
                  min.reads = 100,
                  min.lib.size = 100,
                  od.genes.alpha = 0.10)

```

```{r}

start_time <- Sys.time()

pdac_a1_LDAs <- fitLDA(pdac_a1$slm, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

```{r}

pdac_a1_opt <- buildLDAobject(LDAmodel = optimalModel(pdac_a1_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

```


# PDAC A ST2

```{r}

path <- "/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/moncada_pdac_GSE111672/GSE111672_RAW/GSM4100721_PDAC-A-st2.tsv"

# input for `preprocess` should be spot (row) x gene (columns) mtx with raw gene counts

pdac_a2 <- loadPDACfile(path = path)
pdac_a2[1:10,1:10]

```

```{r}

pdac_a2 <- preprocess(pdac_a2,
                  alignFile = NA, extractPos = TRUE,
                  nTopGenes = 5,
                  genes.to.remove = c("^MT", "^RPL", "^MRPL"), # note, MTOR is removed...maybe other non MT genes...?
                  perc.spots = .95,
                  min.reads = 100,
                  min.lib.size = 100,
                  od.genes.alpha = 0.10)

```

```{r}

start_time <- Sys.time()

pdac_a2_LDAs <- fitLDA(pdac_a2$slm, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

```{r}

pdac_a2_opt <- buildLDAobject(LDAmodel = optimalModel(pdac_a2_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

```

# PDAC B ST1

```{r}

path <- "/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/moncada_pdac_GSE111672/GSE111672_RAW/GSM3405534_PDAC-B-ST1-filtered.txt"

# input for `preprocess` should be spot (row) x gene (columns) mtx with raw gene counts

pdac_b1 <- loadPDACfile(path = path)
pdac_b1[1:10,1:10]

```

```{r}

pdac_b1 <- preprocess(pdac_b1,
                  alignFile = NA, extractPos = TRUE,
                  nTopGenes = 5,
                  genes.to.remove = c("^MT", "^RPL", "^MRPL"), # note, MTOR is removed...maybe other non MT genes...?
                  perc.spots = .95,
                  min.reads = 100,
                  min.lib.size = 100,
                  od.genes.alpha = 0.10)

```

```{r}

start_time <- Sys.time()

pdac_b1_LDAs <- fitLDA(pdac_b1$slm, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

```{r}

pdac_b1_opt <- buildLDAobject(LDAmodel = optimalModel(pdac_b1_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

```

# PDAC B ST2

```{r}

path <- "/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/moncada_pdac_GSE111672/GSE111672_RAW/GSM4100723_PDAC-B-st2.tsv"

# input for `preprocess` should be spot (row) x gene (columns) mtx with raw gene counts

pdac_b2 <- loadPDACfile(path = path)
pdac_b2[1:10,1:10]

```

```{r}

pdac_b2 <- preprocess(pdac_b2,
                  alignFile = NA, extractPos = TRUE,
                  nTopGenes = 5,
                  genes.to.remove = c("^MT", "^RPL", "^MRPL"), # note, MTOR is removed...maybe other non MT genes...?
                  perc.spots = .95,
                  min.reads = 100,
                  min.lib.size = 100,
                  od.genes.alpha = 0.10)

```

```{r}

start_time <- Sys.time()

pdac_b2_LDAs <- fitLDA(pdac_b2$slm, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

```{r}

pdac_b2_opt <- buildLDAobject(LDAmodel = optimalModel(pdac_b2_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

```

# ==========================
# MERFISH

## data
## '171021_FN7_2_M22_M26'

```{r}

load("/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/mpoa_merfish_clean.RData")
# has `annot.table`, `counts`, and `features`
# annot.table: table of the individual cells and data like coordinates, cell types, bregma, animal
# features: has cells and additional features. Also dataset they belong to
# counts: gene counts of cell in annot.table for 130 merfish genes profiled

# select cells that are part of given dataset:
selected_cells <- rownames(features)[features$dataset_name %in% c('171021_FN7_2_M22_M26')]

spatial_position_and_class <- annot.table[selected_cells, c('Centroid_X', 'Centroid_Y', 'Bregma', "Cell_class", "Neuron_cluster_ID")]
spatial_position_and_class <- na.omit(spatial_position_and_class) # remove rows with NA

dim(spatial_position_and_class)
# [1] 36329     5

```

```{r}

FN7_2_M22_M26_hash <- build_bregma_hash_table(spatial_position_and_class, 100)

```

```{r}

keys(FN7_2_M22_M26_hash)

```

# models

to run through 14 k's:
seq(from = 10, to = 75, by = 5)
takes on average 15 minutes for the MERFISH corpuses

```{r}

# documents and genes same for all:
sapply(bregma04_LDAs$models, slot, "Dim")[,1]

# terms in each corpus
sapply(bregma04_LDAs$models, slot, "n")[1]
sapply(bregma09_LDAs$models, slot, "n")[1]
sapply(bregma14_LDAs$models, slot, "n")[1]
sapply(bregma19_LDAs$models, slot, "n")[1]
sapply(bregma24_LDAs$models, slot, "n")[1]
sapply(bregma29_LDAs$models, slot, "n")[1]

# about 1.4 million terms

```
bregmas:
<!-- "-0.04" "-0.09" "-0.14" "-0.19" "-0.24" "-0.29" -->

```{r}

load("/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/merfish_FN7_2_M22_M26_hash.RData")

```

# -0.04

```{r}

bregma04 <- extractBregmaCorpus(FN7_2_M22_M26_hash, "-0.04")

```

```{r, fig.height=6, fig.width=8}

breg <- bregma04

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  facet_wrap(~ Cell_class, nrow = 4) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df, aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols) +
  geom_rect(data = breg$gtDocTopics,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100),
            fill = NA, color = "black")

ggplot() +
  geom_point(data = breg$cellCounts, size=7, aes(x=x+50, y=y+50, color=counts)) +
  scale_color_viridis(option = "C") +
  geom_rect(data = breg$cellCounts,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100), 
            fill = NA, color = "black")

```

```{r}

start_time <- Sys.time()

bregma04_LDAs <- fitLDA(bregma04$sim, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

# -0.09

```{r}

bregma09 <- extractBregmaCorpus(FN7_2_M22_M26_hash, "-0.09")

```

```{r, fig.height=6, fig.width=8}

breg <- bregma09

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  facet_wrap(~ Cell_class, nrow = 4) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df, aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols) +
  geom_rect(data = breg$gtDocTopics,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100),
            fill = NA, color = "black")

ggplot() +
  geom_point(data = breg$cellCounts, size=7, aes(x=x+50, y=y+50, color=counts)) +
  scale_color_viridis(option = "C") +
  geom_rect(data = breg$cellCounts,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100), 
            fill = NA, color = "black")

```

```{r}

start_time <- Sys.time()

bregma09_LDAs <- fitLDA(bregma09$sim, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

# -0.14

```{r}

bregma14 <- extractBregmaCorpus(FN7_2_M22_M26_hash, "-0.14")

```

```{r, fig.height=6, fig.width=8}

breg <- bregma14

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  facet_wrap(~ Cell_class, nrow = 4) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df, aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols) +
  geom_rect(data = breg$gtDocTopics,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100),
            fill = NA, color = "black")

ggplot() +
  geom_point(data = breg$cellCounts, size=7, aes(x=x+50, y=y+50, color=counts)) +
  scale_color_viridis(option = "C") +
  geom_rect(data = breg$cellCounts,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100), 
            fill = NA, color = "black")

```

```{r}

start_time <- Sys.time()

bregma14_LDAs <- fitLDA(bregma14$sim, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

# -0.19

```{r}

bregma19 <- extractBregmaCorpus(FN7_2_M22_M26_hash, "-0.19")

```

```{r, fig.height=6, fig.width=8}

breg <- bregma19

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  facet_wrap(~ Cell_class, nrow = 4) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df, aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols) +
  geom_rect(data = breg$gtDocTopics,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100),
            fill = NA, color = "black")

ggplot() +
  geom_point(data = breg$cellCounts, size=7, aes(x=x+50, y=y+50, color=counts)) +
  scale_color_viridis(option = "C") +
  geom_rect(data = breg$cellCounts,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100), 
            fill = NA, color = "black")

```

```{r}

start_time <- Sys.time()

bregma19_LDAs <- fitLDA(bregma19$sim, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

# -0.24

```{r}

bregma24 <- extractBregmaCorpus(FN7_2_M22_M26_hash, "-0.24")

```

```{r, fig.height=6, fig.width=8}

breg <- bregma24

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  facet_wrap(~ Cell_class, nrow = 4) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df, aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols) +
  geom_rect(data = breg$gtDocTopics,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100),
            fill = NA, color = "black")

ggplot() +
  geom_point(data = breg$cellCounts, size=7, aes(x=x+50, y=y+50, color=counts)) +
  scale_color_viridis(option = "C") +
  geom_rect(data = breg$cellCounts,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100), 
            fill = NA, color = "black")

```

```{r}

start_time <- Sys.time()

bregma24_LDAs <- fitLDA(bregma24$sim, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

# -0.29

```{r}

bregma29 <- extractBregmaCorpus(FN7_2_M22_M26_hash, "-0.29")

```

```{r, fig.height=6, fig.width=8}

breg <- bregma29

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  facet_wrap(~ Cell_class, nrow = 4) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df,
             aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols)

ggplot() +
  geom_point(data = breg$df, aes(x=Centroid_X, y=Centroid_Y, color=Cell_class)) +
  scale_fill_manual(values=breg$cols) +
  geom_rect(data = breg$gtDocTopics,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100),
            fill = NA, color = "black")

ggplot() +
  geom_point(data = breg$cellCounts, size=7, aes(x=x+50, y=y+50, color=counts)) +
  scale_color_viridis(option = "C") +
  geom_rect(data = breg$cellCounts,
            aes(xmin = x - 0, xmax = x + 100,
                ymin = y - 0, ymax = y + 100), 
            fill = NA, color = "black")

```

```{r}

start_time <- Sys.time()

bregma29_LDAs <- fitLDA(bregma29$sim, k_, seed = 0)

total_t <- round(difftime(Sys.time(), start_time, units = "mins"), 2)
print(sprintf("Time to train LDA models was %smins", total_t))

```

# compare results

```{r}

bregmaOpt_04 <- buildLDAobject(LDAmodel = optimalModel(bregma04_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

bregmaOpt_09 <- buildLDAobject(LDAmodel = optimalModel(bregma09_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

bregmaOpt_14 <- buildLDAobject(LDAmodel = optimalModel(bregma14_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

bregmaOpt_24 <- buildLDAobject(LDAmodel = optimalModel(bregma24_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

bregmaOpt_29 <- buildLDAobject(LDAmodel = optimalModel(bregma29_LDAs),
                      deepSplit = 4,
                      colorScheme = "ggplot")

```

## 04 vs 09

```{r}

beta_04v09 <- correlationBetweenBetas(beta1 = bregmaOpt_04$beta,
                                      beta2 = bregmaOpt_09$beta)

betaCombn_04v09 <- correlationBetweenBetas(beta1 = bregmaOpt_04$betaCombn,
                                       beta2 = bregmaOpt_09$betaCombn)

```

```{r, fig.height=7, fig.width=8}

par(mfrow=c(1,1), mar=c(8,8,3,2))
heatmap.2(beta_04v09,
          Rowv = bregmaOpt_04$dendro,
          Colv = bregmaOpt_09$dendro,
          density.info = "none",
          trace = "none",
          RowSideColors = as.vector(bregmaOpt_04$cols),
          ColSideColors = as.vector(bregmaOpt_09$cols),
          col = correlation_palette,
          breaks = correlation_breaks,
          cexRow=1,cexCol=1,margins=c(6,3),
          main = "beta_04v09",
          lhei = c(1,5),
          key.xlab = "Correlation",
          key.title = NA)

pairs <- lsatPairs(beta_04v09)
print(pairs)

# recall the individual topics colored by their cluster.
# when ordered based on matches and not dendro, colors will appear mixed
par(mfrow=c(1,1), mar=c(8,8,3,2))
heatmap.2(beta_04v09[pairs$rowix, pairs$colsix],
          Rowv = NULL,
          Colv = NULL,
          density.info = "none",
          trace = "none",
          RowSideColors = as.vector(bregmaOpt_04$cols)[pairs$rowix],
          ColSideColors = as.vector(bregmaOpt_09$cols)[pairs$colsix], # order now based on lsat
          col = correlation_palette,
          breaks = correlation_breaks,
          cexRow=1,cexCol=1,margins=c(6,3),
          main = "beta_04v09",
          lhei = c(1,5),
          key.xlab = "Correlation",
          key.title = NA)

# correlations along diagonal (after assigning best matches)
beta_04v09_cors <- diag(beta_04v09[pairs$rowix, pairs$colsix])

# -------------------------------------------------------
# betaCombn:

par(mfrow=c(1,1), mar=c(8,8,3,2))
heatmap.2(betaCombn_04v09,
          # Rowv = bregmaOpt_04$dendro,
          # Colv = bregmaOpt_09$dendro,
          density.info = "none",
          trace = "none",
          RowSideColors = as.vector(bregmaOpt_04$clustCols),
          ColSideColors = as.vector(bregmaOpt_09$clustCols),
          col = correlation_palette,
          breaks = correlation_breaks,
          cexRow=1,cexCol=1,margins=c(6,3),
          main = "betaCombn_04v09",
          lhei = c(1,5),
          key.xlab = "Correlation",
          key.title = NA)

pairs <- lsatPairs(t(betaCombn_04v09)) # 04 (rows) has 23 clusters and 09 has 22 (cols) to need to t()

par(mfrow=c(1,1), mar=c(8,8,3,2))
heatmap.2(betaCombn_04v09[pairs$colsix, pairs$rowix], # note for pairing it was t() so switch ix's
          Rowv = NULL,
          Colv = NULL,
          density.info = "none",
          trace = "none",
          RowSideColors = as.vector(bregmaOpt_04$clustCols)[pairs$colsix], # order based on lsat, and bc more rows than cols in the mtx, it was first t(). And thus not all of the original rows were paired
          ColSideColors = as.vector(bregmaOpt_09$clustCols),
          col = correlation_palette,
          breaks = correlation_breaks,
          cexRow=1,cexCol=1,margins=c(6,3),
          main = "betaCombn_04v09",
          lhei = c(1,5),
          key.xlab = "Correlation",
          key.title = NA)

# correlations along diagonal (after assigning best matches)
betaCombn_04v09_cors <- diag(betaCombn_04v09[pairs$colsix, pairs$rowix])

```

```{r}

hist(beta_04v09_cors, breaks = 20)
mean(beta_04v09_cors)
sd(beta_04v09_cors)

hist(betaCombn_04v09_cors, breaks = 20)
mean(betaCombn_04v09_cors)
sd(betaCombn_04v09_cors)

```

# ==========================
# SPOTlight

# mOB scRNAseq reference

starting from processed:

```{r}

# mob_se <- read.csv2("/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/GSE121891_mOB_scRNAseq/GSE121891_OB_6_runs_processed_seurat.dge.csv", sep = ",")
# 
# mob_se <- as.sparse(mob_se)

```

```{r}

# load("/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/GSE121891_mOB_scRNAseq/mob_processed_sparseMatrix.RData")
# mob_se

```

```{r}

# mob_meta <- read.csv2("/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/GSE121891_mOB_scRNAseq/GSE121891_OB_metaData_seurat.csv", sep = ",")

```

Use metadata to just get the WT cells and the associated clusters from the metadata.
Make sure the metadata cell labels selected and the cells in the count matrix match.
Then make the seurat object. Might have to change some settings in the object to get the variable genes. And a way to get the variable features out of the seurat object.

```{r}

# wt_cells <- mob_meta[which(mob_meta$orig.ident %in% c("WT1", "WT2")),]$"X"
# print(length(wt_cells))

```

```{r}

# mob_se_wt <- mob_se[,wt_cells]

```

```{r}

# mob_se_wt <- CreateSeuratObject(counts = mob_se_wt, project = "mob",
                                meta.data = mob_meta[which(mob_meta$orig.ident %in% c("WT1", "WT2")),])

```

```{r}

# a lot of metadata info not being added...? try doing manually:
# mob_se_wt[["ClusterName"]] <- mob_meta[which(mob_meta$orig.ident %in% c("WT1", "WT2")),]$ClusterName
# mob_se_wt[["percent.mito"]] <- mob_meta[which(mob_meta$orig.ident %in% c("WT1", "WT2")),]$percent.mito

```

based on the paper methods and the values, it looks already log normalized and filtered.

the spotlight pipeline is looking for "assay = RNA" and "slot = counts" so make sure these are part of the seurat object.

For PCA and UMAP, idea is to get clusters. But I already have clusters in the meta data file. So can skip this.

If I wanted to do PCA, this is looking for "scale.data" slot, and to get this I can do:
ScaleData,

or alternatively, I can do SCTransform, which is an alternative to: 1. Normalize, 2. Find Variable Features, 3. Scale data.

Results are saved in a new assay (named SCT by default) with counts being (corrected) counts, data being log1p(counts), and scale.data is the pearson residuals.

But because this data has already been log transformed, and I'm not sure how SCTransform is transforming the data, I should just stick to finding the variable features and then scaling.

But scaling takes time, and again, I already have clusters for cells from metadata. So skip for now.

```{r}

# mob_se_wt <- FindVariableFeatures(mob_se_wt, selection.method = "vst", nfeatures = 3000)
# mob_se_wt <- ScaleData(mob_se_wt, vars.to.regress = c("percent.mito", "nCount_RNA"))

```

```{r}

# mob_se_wt <- RunPCA(mob_se_wt, verbose = FALSE)
# mob_se_wt <- RunUMAP(mob_se_wt, dims = 1:30, verbose = FALSE)
# 
# Seurat::DimPlot(cortex_sc,
#                 group.by = "ClusterName",
#                 label = TRUE) + Seurat::NoLegend()

```

Set the known clusters as the "active.indent"

Then get the markers for each of these clusters. In this case, using the already normalized data.

It looks like under the only assay I have so far, RNA, there is counts and data. But by the looks of it both are the same values so doesnt matter which one is used. In the future, data is probably transformed and counts could be adjusted or maybe raw counts.

```{r}

# Seurat::Idents(object = mob_se_wt) <- mob_se_wt@meta.data$ClusterName
# 
# cluster_markers_mob_se_wt <- Seurat::FindAllMarkers(object = mob_se_wt, 
#                                               assay = "RNA",
#                                               slot = "data",
#                                               verbose = TRUE, 
#                                               only.pos = TRUE)

```

Now we can train the NMF model:

```{r}

# se_sc_down <- downsample_se_obj(se_obj = mob_se_wt,
#                                 clust_vr = "ClusterName",
#                                 cluster_markers = cluster_markers_mob_se_wt,
#                                 cl_n = 100, # size to sample from each cluster. Cluster types determine from meta.data in se object via clust_vr 
#                                 hvg = 3000) # number or NULL. choose num of highly variable genes

```

```{r}

# start_time <- Sys.time()
# nmf_mod_mob_wt_ls <- train_nmf(cluster_markers = cluster_markers_mob_se_wt, 
#                         se_sc = se_sc_down, 
#                         mtrx_spatial = cd, # the original mob st data
#                         clust_vr = "subclass",
#                         ntop = NULL,
#                         hvg = 3000, # number of highly variable genes that is used in addition to the marker genes
#                         transf = "uv",
#                         method = "nsNMF")
# 
# nmf_mod_mob_wt <- nmf_mod_mob_wt_ls[[1]]

# [1] "Preparing Gene set"
# [1] "Normalizing count matrix"
# [1] "Seeding initial matrices"
# [1] "Training..."
# [1] "Time to train NMF model was 255.28mins"

```

```{r}

# load("/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/modeling_merged.20210209.image.RData")
# load("/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/GSE121891_mOB_scRNAseq/spotlight_nmf_train_with_mob.RData")
# 
# nmf_mod_mob_wt <- nmf_mod_mob_wt_ls[[1]]


# this should have all the previous variables and outputs from the LDA models
# plus the SPOTlight NMF outputs after training
load("/Users/brendan/Desktop/PostDoc/work/STDeconvolve/data/modeling_merged.20210211.image.RData")

```


```{r}

# get basis matrix W (genes x topic)
w <- basis(nmf_mod_mob_wt)
dim(w)

# get coefficient matrix H (cells x topic)
h <- coef(nmf_mod_mob_wt)
dim(h)

```

```{r}

# reference for which cell type(s) a topic represents
ct_topic_profiles <- topic_profile_per_cluster_nmf(h = h,
                              train_cell_clust = nmf_mod_mob_wt_ls[[2]])

```

adjust the counts of the mob data:
```{r, mob-qc, fig.width=8, fig.height=3}

# Remove poor datasets and genes
# cd_clean <- MERINGUE::cleanCounts(counts = cd, 
#                       min.reads = 100, 
#                       min.lib.size = 100, 
#                       plot=TRUE,
#                       verbose=TRUE)
# posMob <- pos[colnames(cd_clean),]

# CPM normalize
cd_cpm_log <- MERINGUE::normalizeCounts(counts = cd, 
                       log=TRUE,
                       verbose=TRUE)

# NOTE: because the NMF model was built using the starting cd, if filtering it, it won't work if some genes are removed. Because the matrix W was made with the set of genes that was the intersection of `cd` and also the seurat downsampled object. If this changes, then can't do the nnls step. Matrices will be incomparable sizes

# however, should likely make sure the counts are normalized similarly to how the single cell data was when determining cluster marker genes.

```

```{r}

mixture_transcriptome <- cd_cpm_log
transf <- "uv" # normalization for the st data. note same as what was done for the scRNAseq data

profile_mtrx <- predict_spatial_mixtures_nmf(nmf_mod = nmf_mod_mob_wt,
                               mixture_transcriptome = mixture_transcriptome,
                               transf = transf)

```

#### With raw 'cd'

NOTE:
if I use the `cd` with raw counts, 23 topics are kept when visualizing.
HOWEVER, most are just a couple spots. So maybe suggests that noise is being picked up because data wasn't normalized and log scaled.

If I use the cpm log normalized data, then only 11 topics are kept and look like layers. But I will note that the LDA actually looks like it's capturing more information...

```{r}

decon_mtrx <- mixture_deconvolution_nmf(nmf_mod = nmf_mod_mob_wt,
                          mixture_transcriptome = cd,
                          transf = "uv", 
                          reference_profiles = ct_topic_profiles, 
                          min_cont = 0.09)

rownames(decon_mtrx) <- colnames(mixture_transcriptome)
spotlightPredictions_1 <- decon_mtrx[,1:(ncol(decon_mtrx)-1)] # last column is residuals

# drop any clusters that were not detected at all in the ST data
filtPredictions_1 <- spotlightPredictions_1[,which(!colSums(spotlightPredictions_1) == 0)]

# assign colors for the remaining spotlight predicted clusters that are present
deconClusterCols_1 <- as.factor(colnames(filtPredictions_1))
names(deconClusterCols_1) <- colnames(filtPredictions_1)
levels(deconClusterCols_1) <- rainbow(length(colnames(filtPredictions_1)))

```

ALSO might be possible to just use subsets of the genes in W.
because it can be accessed via:
nmf_mod@fit@W

And I think basis is just a wrapper to return this matrix

H also accessed the same way. I think coef also just returns this, too



```{r, fig.height=6, fig.width=8}

vizAllTopics(theta = filtPredictions_1,
             pos = pos,
             topicOrder = seq_len(length(colnames(filtPredictions_1))),
             cluster_cols = levels(deconClusterCols_1),
             groups = NA,
             group_cols = NA, 
             r = 0.4,
             lwd = 0.01,
             plotTitle = "decon_mtrx mob sc ref")

vizTopicClusters(theta = filtPredictions_1,
                pos = pos,
                clusters = deconClusterCols_1,
                sharedCol = TRUE,
                r = 0.4,
                lwd = 0.01)

# cols <- rep("gray", length(levels(deconClusterCols)))
# cols[c(1,2,6,13)] <- levels(deconClusterCols)[c(1,2,6,13)]
# vizAllTopics(theta = filtPredictions,
#              pos = pos,
#              topicOrder = seq_len(length(colnames(filtPredictions))),
#              cluster_cols = cols,
#              groups = NA,
#              group_cols = NA,
#              r = 0.4,
#              lwd = 0.01,
#              plotTitle = "decon_mtrx cortex ref")

# cols <- rep("gray", length(levels(deconClusterCols)))
# cols[c(2,13)] <- levels(deconClusterCols)[c(2,13)]
# vizAllTopics(theta = filtPredictions,
#              pos = pos,
#              topicOrder = seq_len(length(colnames(filtPredictions))),
#              cluster_cols = cols,
#              groups = NA,
#              group_cols = NA,
#              r = 0.4,
#              lwd = 0.01,
#              plotTitle = "decon_mtrx cortex ref")

```

```{r}

spotlight_beta_1 <- w
colnames(spotlight_beta_1) <- colnames(spotlightPredictions_1)
spotlight_beta_1 <- t(spotlight_beta_1)

```

```{r}

spotlight_beta_compare_1 <- spotlight_beta_1[colnames(spotlight_samePos_1),which(colnames(spotlight_beta_1) %in% colnames(mob.k30$beta))]

mob_beta_compare_1 <- mob.k30$beta[,colnames(spotlight_beta_compare_1)]

mob_beta_compareCombn_1 <- mob.k30$betaCombn[,colnames(spotlight_beta_compare_1)]

```

taking subsets of the genes from each beta (the ones on both betas)
So adjust probabilities such that they add up to 1 relative to the subset of genes

```{r}

spotlight_beta_compare_adj_1 <- base::do.call(rbind, (lapply(rownames(spotlight_beta_compare_1), function(x){
  v <- spotlight_beta_compare_1[x,]
  adj <- v/sum(v)
  adj
})))
rownames(spotlight_beta_compare_adj_1) <- rownames(spotlight_beta_compare_1)


mob_beta_compare_adj_1 <- base::do.call(rbind, (lapply(rownames(mob_beta_compare_1), function(x){
  v <- mob_beta_compare_1[x,]
  adj <- v/sum(v)
  adj
})))
rownames(mob_beta_compare_adj_1) <- rownames(mob_beta_compare_1)


mob_beta_compareCombn_adj_1 <- base::do.call(rbind, (lapply(rownames(mob_beta_compareCombn_1), function(x){
  v <- mob_beta_compareCombn_1[x,]
  adj <- v/sum(v)
  adj
})))
rownames(mob_beta_compareCombn_adj_1) <- rownames(mob_beta_compareCombn_1)

```

```{r}

mob_ldaVsSPOT_betas_1 <- correlationBetweenBetas(beta1 = mob_beta_compare_adj_1,
                         beta2 = spotlight_beta_compare_adj_1)

mob_ldaVsSPOT_betasCombn_1 <- correlationBetweenBetas(beta1 = mob_beta_compareCombn_adj_1,
                         beta2 = spotlight_beta_compare_adj_1)

```

# ==========================
# Additional Questions:

## corpus and optimal k

how does the:
1. size of the corpus (num docs, num genes, num terms)
2. or "complexity" of the corpus

influence the "optimal k"

potentially important because I believe that the models are being over fit to the input corpus and thus the optimal k determined is too large. It also leads to slow down because the larger the k, the more time the model takes to fit.

So if I can figure out what drives the model to find an optimal k and what increases k, I could use this to guide the construction of the input corpus. OD genes? removing genes expressed in all spots? removing genes with high expression to lower the total terms?


## corpus and speed

related to the above question. A larger k certainly makes it slower to fit the model, but perhaps the size or the corpus itself also influences the speed of fitting, regardless of the size of k.


try keeping K constant but modulating the size (doc x genes; also total terms) and measure time to fit. Also perhaps the "complexity". As in, certain topics may share a lot of the same genes instead of topics have unique genes.














